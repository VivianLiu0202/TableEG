import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import numpy as np
from scipy.spatial.distance import jaccard, cosine, jensenshannon
from scipy.stats import entropy, chi2_contingency
from sklearn.metrics import precision_recall_fscore_support
from collections import Counter, defaultdict
import json
import os
from pathlib import Path
import sys

class TableErrorEvaluator:
    def __init__(self, clean_path, dirty_path, verbose=True):
        """
        Initialize the evaluator by loading clean and dirty tables.

        Args:
            clean_path (str): Path to the clean dataset.
            dirty_path (str): Path to the dirty dataset.
            verbose (bool): Whether to enable logging output.
        """
        self.verbose = verbose
        self._log(f"Using clean dataset: {clean_path}", level="info")
        self._log(f"Using dirty dataset: {dirty_path}", level="info")

        self.clean_df = pd.read_csv(clean_path, dtype=str)
        self.dirty_df = pd.read_csv(dirty_path, dtype=str)

        # Set "row" as index to avoid KeyErrors
        self.clean_df.set_index("row", inplace=False)
        self.dirty_df.set_index("row", inplace=False)

        # Extract errors from the dirty dataset
        self.clean_errors = self.extract_errors(self.clean_df, self.dirty_df)

    def _log(self, message, level="info"):
        """
        Print messages only if verbose mode is enabled.

        Args:
            message (str): The message to be logged.
            level (str): Log level ('info', 'warning', 'error').
        """
        if self.verbose:
            prefix = {"info": "[INFO]", "warning": "[WARNING]", "error": "[ERROR]"}.get(level, "[INFO]")
            print(f"{prefix} {message}")

    def extract_errors(self, clean_df, dirty_df):
        """
        Identify errors in the dirty dataset.

        Args:
            clean_df (pd.DataFrame): Clean dataset.
            dirty_df (pd.DataFrame): Dirty dataset.

        Returns:
            dict: Column-wise error counts.
        """
        errors = defaultdict(int)
        for index, row in dirty_df.iterrows():
            index = int(index)
            for col in dirty_df.columns:
                if row[col] != clean_df.at[index, col]:  # Detect errors
                    errors[col] += 1

        self._log(f"Extracted {sum(errors.values())} errors from dirty dataset.", level="info")
        return errors
    
    def extract_errors_bart_or_llm_errors(self, json_path):
        """
        Extract errors from a JSONL file generated by BART or LLM.

        Args:
            json_path (str): Path to the JSONL file.

        Returns:
            dict: Column-wise error counts.
        """
        errors = defaultdict(int)
        with open(json_path, "r", encoding="utf-8-sig") as f:
            for line in f:
                data = json.loads(line.strip())
                column_name = data.get("column")
                if column_name:
                    errors[column_name] += 1

        self._log(f"Extracted {sum(errors.values())} errors from {json_path}.", level="info")
        return errors

    def get_data_type(self, value):
        """
        Determine the data type of a given value.

        Args:
            value (str): Value to classify.

        Returns:
            str: "numeric", "datetime", or "text".
        """
        try:
            float(value)
            return "numeric"
        except ValueError:
            return "datetime" if ":" in value or "-" in value else "text"


    def compute_text_similarity(self, texts1, texts2):
        """
        Compute cosine similarity for text-based errors.

        Args:
            texts1 (list): First set of text values.
            texts2 (list): Second set of text values.

        Returns:
            float: Cosine similarity score.
        """
        if not texts1 or not texts2:
            return 0
        emb1 = self.text_encoder.encode(texts1, convert_to_tensor=True)
        emb2 = self.text_encoder.encode(texts2, convert_to_tensor=True)
        return float(cosine_similarity(emb1.cpu().numpy(), emb2.cpu().numpy()).mean())

    def compute_numeric_similarity(self, values1, values2):
        """
        Compute normalized numeric deviation.

        Args:
            values1 (list): First set of numeric values.
            values2 (list): Second set of numeric values.

        Returns:
            float: Normalized numeric similarity score.
        """
        values1 = np.array([float(v) for v in values1 if v.replace(".", "", 1).isdigit()])
        values2 = np.array([float(v) for v in values2 if v.replace(".", "", 1).isdigit()])

        if values1.size == 0 or values2.size == 0:
            return 0  # Avoid empty data
        scaler = StandardScaler()
        norm_values1 = scaler.fit_transform(values1.reshape(-1, 1))
        norm_values2 = scaler.transform(values2.reshape(-1, 1))
        return 1 - np.abs(norm_values1 - norm_values2).mean()

    def compute_datetime_similarity(self, values1, values2):
        """
        Compute normalized time deviation.

        Args:
            values1 (list): First set of datetime values.
            values2 (list): Second set of datetime values.

        Returns:
            float: Normalized time similarity score.
        """
        try:
            time_diff = np.abs(pd.to_datetime(values1) - pd.to_datetime(values2))
            return 1 - np.mean(time_diff.total_seconds()) / (24 * 3600)  # Normalize by days
        except Exception:
            return 0  # Unable to parse dates, return 0

    def semantic_similarity(self, clean_errors, generated_errors):
        """
        Compute semantic similarity between true and generated errors using LLM embeddings.

        Args:
            clean_errors (set): A set of (row, column) pairs representing actual errors.
            generated_errors (list): A list of tuples containing generated error values.

        Returns:
            float: Weighted semantic similarity score.
        """
        self._log("Computing semantic similarity between clean and generated errors.", level="info")

        # Retrieve true and generated error values
        try:
            true_values = [self.dirty_df.at[row, col] for row, col in clean_errors]
            generated_values = [error_value for _, _, error_value in generated_errors]
        except KeyError as e:
            self._log(f"KeyError encountered while retrieving values: {e}", level="error")
            return 0.0  # Return 0 similarity if indexing fails

        # Categorize errors by data type
        text_errors, numeric_errors, datetime_errors = [], [], []

        for true_val, gen_val in zip(true_values, generated_values):
            data_type = self.get_data_type(true_val)
            if data_type == "text":
                text_errors.append((true_val, gen_val))
            elif data_type == "numeric":
                numeric_errors.append((true_val, gen_val))
            elif data_type == "datetime":
                datetime_errors.append((true_val, gen_val))

        self._log(
            f"Text errors: {len(text_errors)}, Numeric errors: {len(numeric_errors)}, Datetime errors: {len(datetime_errors)}",
            level="info")

        # Compute similarity scores for each data type
        text_sim = self.compute_text_similarity(*zip(*text_errors)) if text_errors else 0
        num_sim = self.compute_numeric_similarity(*zip(*numeric_errors)) if numeric_errors else 0
        date_sim = self.compute_datetime_similarity(*zip(*datetime_errors)) if datetime_errors else 0

        # Weighted final similarity score
        final_score = 0.5 * text_sim + 0.3 * num_sim + 0.2 * date_sim
        self._log(f"Final weighted semantic similarity score: {final_score:.4f}", level="info")

        return final_score

    def jaccard_similarity(self, errors1, errors2):
        """
        Compute Jaccard similarity between two error distributions.

        Args:
            errors1 (dict): First set of errors.
            errors2 (dict): Second set of errors.

        Returns:
            float: Jaccard similarity score.
        """
        all_keys = set(errors1.keys()).union(errors2.keys())
        vec1 = np.array([errors1.get(key, 0) for key in all_keys], dtype=float)
        vec2 = np.array([errors2.get(key, 0) for key in all_keys], dtype=float)

        prob1 = vec1 / vec1.sum()
        prob2 = vec2 / vec2.sum()

        intersection = (np.minimum(prob1, prob2))
        union = (np.maximum(prob1, prob2))

        res = intersection / (union +1e-10)

        return np.mean(res)

    def exact_match_score(self, generated_errors):
        """
        Compute exact match accuracy.

        Args:
            generated_errors (set): Set of generated errors (row, column) pairs.

        Returns:
            float: Exact match accuracy.
        """
        if not self.clean_errors:
            self._log("No clean errors available for comparison.", level="warning")
            return 0.0

        matches = len(self.clean_errors.intersection(generated_errors))
        accuracy = matches / len(self.clean_errors)

        self._log(f"Exact match score: {accuracy:.4f} ({matches}/{len(self.clean_errors)} matches found)", level="info")
        return accuracy

    def f1_score(self, generated_errors):
        """
        Compute F1-score for generated errors.

        Args:
            generated_errors (set): Set of generated errors (row, column) pairs.

        Returns:
            tuple: (precision, recall, F1-score)
        """
        if not self.clean_errors:
            self._log("No clean errors available for F1-score calculation.", level="warning")
            return 0.0, 0.0, 0.0

        # Convert errors into binary classification labels
        y_true = [1 if e in self.clean_errors else 0 for e in generated_errors]
        y_pred = [1] * len(generated_errors)  # Assume all generated errors are positives

        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average="binary", zero_division=0
        )

        self._log(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}", level="info")
        return precision, recall, f1

    def kl_divergence(self, clean_distribution, generated_distribution):
        """
        Compute Kullback-Leibler (KL) divergence between clean and generated error distributions.

        Args:
            clean_distribution (dict): Frequency distribution of clean errors.
            generated_distribution (dict): Frequency distribution of generated errors.

        Returns:
            float: KL divergence score.
        """
        if not clean_distribution or not generated_distribution:
            self._log("One or both distributions are empty. Returning KL divergence as 0.", level="warning")
            return 0.0

        # Convert distributions to probability distributions
        clean_probs = np.array(list(clean_distribution.values()), dtype=np.float64)
        generated_probs = np.array(list(generated_distribution.values()), dtype=np.float64)

        # Normalize probabilities
        clean_probs /= clean_probs.sum()
        generated_probs /= generated_probs.sum()

        # Compute KL divergence
        kl_score = entropy(clean_probs, generated_probs, base=2)

        self._log(f"KL Divergence: {kl_score:.4f}", level="info")
        return kl_score

    def js_divergence(self, clean_errors, generated_errors):
        """
        Compute Jensen-Shannon divergence between two error distributions.

        Args:
            clean_errors (dict): Ground truth errors.
            generated_errors (dict): Generated errors.

        Returns:
            float: Jensen-Shannon divergence score.
        """
        prob1, prob2 = [], []
        all_keys = set(clean_errors.keys()).union(generated_errors.keys())

        for key in all_keys:
            prob1.append(clean_errors.get(key, 0))
            prob2.append(generated_errors.get(key, 0))

        prob1 = np.array(prob1) / sum(prob1)
        prob2 = np.array(prob2) / sum(prob2)

        return jensenshannon(prob1, prob2)

    def row_col_jaccard(self, clean_errors, generated_errors, axis="row"):
        """
        Compute Jaccard similarity for row/column-level error distributions.

        Args:
            clean_errors (set): Set of (row, column) pairs representing clean errors.
            generated_errors (set): Set of (row, column) pairs representing generated errors.
            axis (str): "row" to compare row-level errors, "column" for column-level errors.

        Returns:
            float: Jaccard similarity score.
        """
        if not clean_errors or not generated_errors:
            self._log("One or both error sets are empty. Returning Jaccard similarity as 0.", level="warning")
            return 0.0

        if axis not in {"row", "column"}:
            self._log(f"Invalid axis '{axis}' provided. Defaulting to 'row'.", level="warning")
            axis = "row"

        # Extract row-wise or column-wise error sets
        clean_set = {e[0] for e in clean_errors} if axis == "row" else {e[1] for e in clean_errors}
        generated_set = {e[0] for e in generated_errors} if axis == "row" else {e[1] for e in generated_errors}

        # Compute Jaccard similarity
        jaccard_score = self.jaccard_similarity(clean_set, generated_set)
        self._log(f"Jaccard similarity ({axis}-level): {jaccard_score:.4f}", level="info")

        return jaccard_score

    def chi_square_test(self, clean_errors, generated_errors):
        """
        Perform chi-square test to compare error distributions.

        Args:
            clean_errors (set): Set of (row, column) pairs representing clean errors.
            generated_errors (set): Set of (row, column) pairs representing generated errors.

        Returns:
            tuple: (Chi-square statistic, p-value)
        """
        if not clean_errors or not generated_errors:
            self._log("One or both error sets are empty. Returning chi-square test as (0, 1).", level="warning")
            return 0.0, 1.0

        # Count occurrences of errors in each column
        clean_counts = Counter(col for _, col in clean_errors)
        generated_counts = Counter(col for _, col in generated_errors)

        # Ensure both distributions cover the same set of columns
        unique_cols = set(clean_counts.keys()).union(set(generated_counts.keys()))

        clean_distribution = [clean_counts.get(col, 0) for col in unique_cols]
        generated_distribution = [generated_counts.get(col, 0) for col in unique_cols]

        try:
            chi2, p_value, _, _ = chi2_contingency([clean_distribution, generated_distribution])
            self._log(f"Chi-square statistic: {chi2:.4f}, p-value: {p_value:.4f}", level="info")
        except ValueError as e:
            self._log(f"Chi-square test failed: {e}", level="error")
            return 0.0, 1.0  # Return default values in case of failure

        return chi2, p_value

    def evaluate(self, generate_json_path, BART_generated_clean_path, BART_generated_dirty_path, cal_type="typical"):
        """
        Evaluate all error metrics for LLM-generated and BART-generated errors.

        Args:
            generate_json_path (str): Path to the LLM-generated errors JSON file.
            BART_generated_clean_path (str): Path to the clean dataset generated by BART.
            BART_generated_dirty_path (str): Path to the dirty dataset generated by BART.
            cal_type (str): Type of evaluation ("typical" or "from_cell_changes").

        Returns:
            tuple: (LLM evaluation results, BART evaluation results)
        """
        self._log(f"Evaluating using LLM JSON path: {generate_json_path}", level="info")
        self._log(f"Evaluating using BART clean path: {BART_generated_clean_path}", level="info")
        self._log(f"Evaluating using BART dirty path: {BART_generated_dirty_path}", level="info")
        self._log(f"Evaluation type: {cal_type}", level="info")

        # Extract errors from LLM-generated JSON
        self.llm_gene_errors = self.extract_errors_bart_or_llm_errors(generate_json_path)

        # Extract errors from BART-generated data
        if cal_type == "typical":
            BART_generated_clean = pd.read_csv(BART_generated_clean_path, dtype=str)
            BART_generated_dirty = pd.read_csv(BART_generated_dirty_path, dtype=str)

            BART_generated_clean.set_index("row_id", inplace=False)
            BART_generated_dirty.set_index("row_id", inplace=False)

            self.bart_gene_errors = self.extract_errors(BART_generated_clean, BART_generated_dirty)
            self._log(f"Extracted errors from BART-generated datasets.", level="info")

        elif cal_type == "from_cell_changes":
            try:
                cell_change_path = BART_generated_dirty_path.replace("dirty_data.csv","cellChanges.csv")
                cell_change = pd.read_csv(cell_change_path, dtype=str,header=None)
                t = cell_change[0].str.split(".",expand=True)
                t = t.rename(columns={0:"row_id",1:"column"})
                unique_values_count = t["column"].value_counts().to_dict()

                self.bart_gene_errors = unique_values_count
                self._log(f"Extracted errors from cell changes file: {cell_change_path}", level="info")
            except Exception as e:
                self._log(f"Error processing cell changes file: {e}", level="error")
                return {}, {}

        # Compute evaluation metrics
        results_llm = {
            "Jaccard Similarity": self.jaccard_similarity(self.clean_errors, self.llm_gene_errors),
            "Jensen-Shannon Divergence": self.js_divergence(self.clean_errors, self.llm_gene_errors)
        }

        results_bart = {
            "Jaccard Similarity": self.jaccard_similarity(self.clean_errors, self.bart_gene_errors),
            "Jensen-Shannon Divergence": self.js_divergence(self.clean_errors, self.bart_gene_errors)
        }

        self._log(f"LLM Evaluation Results: {results_llm}", level="info")
        self._log(f"BART Evaluation Results: {results_bart}", level="info")

        return results_llm, results_bart

    def load_generated_errors(self, jsonl_path):
        """
        Load model-generated errors from a JSONL file.

        Args:
            jsonl_path (str): Path to the JSONL file containing generated errors.

        Returns:
            set: A set containing tuples of (row_id, column) for each detected error.
        """
        self._log(f"Loading generated errors from: {jsonl_path}", level="info")
        generated_errors = set()
        try:
            with open(jsonl_path, "r", encoding="utf-8-sig") as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        row_id = entry.get("row_id")
                        column = entry.get("column")

                        if row_id is not None and column is not None:
                            generated_errors.add((row_id, column))

                    except json.JSONDecodeError:
                        self._log(f"JSON decoding failed for line: {line.strip()}", level="error")

            self._log(f"Successfully loaded {len(generated_errors)} generated errors.", level="info")

        except FileNotFoundError:
            self._log(f"File not found: {jsonl_path}", level="error")
        except Exception as e:
            self._log(f"Unexpected error while loading generated errors: {e}", level="error")

        return generated_errors


if __name__ == "__main__":
    # Define base directory relative to the script location
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    BASE_DIR = os.path.abspath(os.path.join(BASE_DIR, ".."))
    # ROOT_DIR = Path(__file__).resolve().parent.parent
    # sys.path.append(str(BASE_DIR))

    # Define relative paths
    BART_root = os.path.join(BASE_DIR, "evaluation/test_dataset/exp_1/BART_output")
    LLM_root = os.path.join(BASE_DIR, "evaluation/test_dataset/exp_1/TableEG_output")
    test_data_root = os.path.join(BASE_DIR, "evaluation/test_dataset/exp_1/raw_test_dataset")
    train_data_root = os.path.join(BASE_DIR, "source")

    # Specify datasets and evaluation parameters
    datasets = ["flight", "rayyan"]
    radio = "10%"
    dataset = "beers"

    # Define paths for clean and dirty datasets
    clean_path = os.path.join(train_data_root, dataset, "clean.csv")  # Path to clean dataset
    dirty_path = os.path.join(train_data_root, dataset, "dirty.csv")  # Path to dirty dataset

    # Define paths for generated error logs
    LLM_generated_jsonl_path = os.path.join(LLM_root, dataset, f"{dataset}_{radio[:-1]}",
                                            f"error_log_{radio[:-1]}.jsonl")
    GPT_generated_jsonl_path = os.path.join(LLM_root.replace("TableEG_output", "GPT_output"), dataset,
                                            f"{dataset}_error_GPT.jsonl")

    # Define paths for BART-generated data
    BART_generated_clean_path = os.path.join(test_data_root, f"clean_{dataset}.csv")  # Path to BART clean dataset
    BART_generated_dirty_path = os.path.join(BART_root, dataset, f"{dataset}_{radio[:-1]}",
                                             "dirty_data.csv")  # Path to BART dirty dataset

    # Initialize the evaluator
    evaluator = TableErrorEvaluator(clean_path, dirty_path)

    # Compute evaluation metrics for LLM and BART
    evaluation_results_llm, evaluation_results_bart = evaluator.evaluate(
        LLM_generated_jsonl_path, BART_generated_clean_path, BART_generated_dirty_path
    )

    # Compute evaluation metrics for GPT
    evaluation_results_gpt, _ = evaluator.evaluate(
        GPT_generated_jsonl_path, BART_generated_clean_path, BART_generated_dirty_path, cal_type="extra"
    )
    # Display evaluation results
    print(f"Table Error Evaluation Results with TableEG on {dataset}_{radio}:")
    print(pd.DataFrame.from_dict(evaluation_results_llm, orient="index", columns=["Score"]))

    print("------------------------------------------------")

    print(f"Table Error Evaluation Results with GPT3.5 on {dataset}_{radio}:")
    print(pd.DataFrame.from_dict(evaluation_results_gpt, orient="index", columns=["Score"]))

    print("------------------------------------------------")

    print(f"Table Error Evaluation Results with BART on {dataset}_{radio}:")
    print(pd.DataFrame.from_dict(evaluation_results_bart, orient="index", columns=["Score"]))

