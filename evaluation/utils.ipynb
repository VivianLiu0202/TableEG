{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils1\n",
    "`Utils1` is a script designed to extract column information for different datasets based on structured annotations. The script reads a JSONL annotation file, identifies datasets based on predefined headers, and extracts the relevant columns from markdown-formatted table data. The extracted column information is then saved in a structured JSON format for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 文件已保存到 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/cols_for_dataset_Marketing.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define base directory relative to the script's location\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Define dataset headers to identify corresponding datasets\n",
    "dataset_headers = {\n",
    "    # \"| row_id | src | flight | sched_dep_time | act_dep_time | sched_arr_time | act_arr_time |\": \"flight\",\n",
    "    # \"| row_id | id | beer-name | style | ounces | abv | ibu | brewery_id | brewery-name | city | state |\": \"beers\",\n",
    "    # \"| row_id | id | article_title | article_language | journal_title | journal_abbreviation | journal_issn | article_jvolumn | article_jissue | article_jcreated_at | article_pagination | author_list |\": \"rayyan\",\n",
    "    # \"| row_id | Id | Name | Year | Release Date | Director | Creator | Actors | Cast | Language | Country | Duration | RatingValue | RatingCount | ReviewCount | Genre | Filming Locations | Description |\": \"movies\",\n",
    "    # \"| row_id | Income | Sex | Marital | Age | Education | Occupation | Live | Dual | Person | Person under 18 | Householder | Hometype | Ethnic | Language |\": \"Marketing\",\n",
    "    \"| row_id | Unnamed: 0 | Date | Latitude | Longitude | Sentiment | Company Name | Country | City | State |\": \"Company\"\n",
    "}\n",
    "\n",
    "# Path to the test annotation file (relative to BASE_DIR)\n",
    "test_annotation_path = os.path.join(BASE_DIR, \"dataset\", \"test\", \"test_ErrorGeneration_zeroshot.jsonl\")\n",
    "\n",
    "# Dictionary to store the columns associated with each dataset\n",
    "cols_for_dataset_with_constraints = defaultdict(set)\n",
    "\n",
    "# Read and process the annotation file\n",
    "with open(test_annotation_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        # Extract table header\n",
    "        header = data[\"input\"].split('\\n')[0]\n",
    "\n",
    "        # If the header matches a known dataset, extract column information\n",
    "        if header in dataset_headers:\n",
    "            markdown_lines = data[\"input\"].split('\\n')[2:]  # Skip header and separator lines\n",
    "            for line in markdown_lines:\n",
    "                cols_for_dataset_with_constraints[dataset_headers[header]].add(line.split(\"|\")[1].strip())\n",
    "\n",
    "# Define output path for storing the extracted columns (relative to BASE_DIR)\n",
    "output_path = os.path.join(BASE_DIR, \"evaluation\", \"test_dataset\", \"exp_1\", \"raw_test_dataset\", \"cols_for_dataset_Marketing.json\")\n",
    "\n",
    "# Convert sets to lists for JSON serialization\n",
    "serializable_dict = {k: list(v) for k, v in cols_for_dataset_with_constraints.items()}\n",
    "\n",
    "# Save the extracted column information to a JSON file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serializable_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"JSON file has been successfully saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils2\n",
    "`Utils2` is a script designed to identify potential rule violations in tabular datasets based on predefined rule descriptions. It cross-references a clean dataset against a set of rules and extracts row pairs that match the rule constraints. The extracted row pairs are stored in a structured JSON format for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 文件已保存到 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/row_pairs_beers.json\n",
      "JSON 文件已保存到 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/row_pairs_flight.json\n",
      "JSON 文件已保存到 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/row_pairs_rayyan.json\n",
      "JSON 文件已保存到 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/row_pairs_restaurant.json\n",
      "JSON 文件已保存到 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/row_pairs_soccer.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the base directory relative to the script location\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"evaluation\", \"test_dataset\", \"exp_1\", \"raw_test_dataset\")\n",
    "\n",
    "# Define datasets to process\n",
    "# datasets = [\"beers\", \"flight\", \"rayyan\", \"restaurant\", \"soccer\"]\n",
    "datasets = [\"movie_metadata\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    rule_file_path = os.path.join(DATASET_DIR, f\"rules_{dataset}.txt\")\n",
    "    clean_file_path = os.path.join(DATASET_DIR, f\"clean_{dataset}.csv\")\n",
    "\n",
    "    # Load the clean dataset\n",
    "    clean_file = pd.read_csv(clean_file_path)\n",
    "    rules = []\n",
    "\n",
    "    # Read the rule description file\n",
    "    with open(rule_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            rules.append(line.strip())\n",
    "\n",
    "    # Load dataset-specific column constraints if applicable\n",
    "    if dataset in [\"beers\", \"flight\", \"rayyan\"]:\n",
    "        cols_list = json.load(open(os.path.join(DATASET_DIR, \"cols_for_dataset.json\"), \"r\"))[dataset]\n",
    "        cols_list = [int(col) for col in cols_list]\n",
    "\n",
    "    # Store row ID pairs that match rule violations\n",
    "    row_pairs = defaultdict(list)\n",
    "\n",
    "    for rule in rules:\n",
    "        t1_fields = []\n",
    "        t2_fields = []\n",
    "\n",
    "        # Parse the rule structure\n",
    "        for condition in rule.split('&'):\n",
    "            symbol = condition.split('(')[0]\n",
    "            t1_field, t2_field = condition[2:].split(',')[0][4:], condition[2:].split(',')[1][3:-1]\n",
    "\n",
    "            if symbol == \"EQ\":\n",
    "                t1_fields.append(t1_field)\n",
    "                t2_fields.append(t2_field)\n",
    "            elif symbol == \"IQ\":\n",
    "                pass\n",
    "            else:\n",
    "                pass  # Ignore unknown symbols\n",
    "\n",
    "        # Ensure matched fields are consistent\n",
    "        assert len(t1_fields) == len(t2_fields), \"t1_fields and t2_fields must have the same length\"\n",
    "        merge_conditions = {t1: t2 for t1, t2 in zip(t1_fields, t2_fields)}\n",
    "\n",
    "        # Perform a self-join on the dataset to find row pairs that match the rules\n",
    "        merged = clean_file.merge(clean_file, left_on=list(merge_conditions.keys()), right_on=list(merge_conditions.values()), suffixes=('_row1', '_row2'))\n",
    "        row_id_pairs = merged[['row_id_row1', 'row_id_row2']].values.tolist()\n",
    "\n",
    "        # Remove self-matching pairs (e.g., [0, 0])\n",
    "        row_id_pairs = [sorted(pair) for pair in row_id_pairs if pair[0] != pair[1]]\n",
    "\n",
    "        # Remove duplicate pairs (e.g., [0, 50] and [50, 0])\n",
    "        row_id_pairs = list(set(tuple(pair) for pair in row_id_pairs))\n",
    "        row_id_pairs = sorted(row_id_pairs)\n",
    "\n",
    "        # Check if the row pairs exist in the dataset constraints\n",
    "        if dataset in [\"beers\", \"flight\", \"rayyan\"]:\n",
    "            for row_pair in row_id_pairs:\n",
    "                row1, row2 = row_pair\n",
    "                if row1 in cols_list and row2 in cols_list:\n",
    "                    row_pairs[rule].append(row_pair)\n",
    "        else:\n",
    "            row_pairs[rule].extend(row_id_pairs)\n",
    "\n",
    "        \n",
    "    # Save results to a JSON file\n",
    "    output_path = os.path.join(DATASET_DIR, f\"row_pairs_{dataset}.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(row_pairs, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"JSON file successfully saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils3\n",
    "`Utils3` is a script designed to generate test datasets by filtering clean data based on row indices extracted from structured annotations. It identifies relevant rows using preprocessed JSON files and creates smaller test datasets for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved test_Company.csv with 497 rows.\n",
      "✅ All filtered datasets have been saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define base directory relative to the script's location\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"evaluation\", \"test_dataset\", \"exp_1\", \"raw_test_dataset\")\n",
    "\n",
    "# Define input file paths\n",
    "json_path = os.path.join(DATASET_DIR, \"cols_for_dataset_Marketing.json\")\n",
    "clean_data_path = os.path.join(DATASET_DIR, \"clean_Company.csv\")\n",
    "\n",
    "# Load row indices from JSON\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    row_indices = json.load(f)  # {\"beers\": [...], \"flight\": [...], \"rayyan\": [...]}\n",
    "\n",
    "# Load the clean dataset\n",
    "clean_data = pd.read_csv(clean_data_path, dtype=str)\n",
    "# Convert row_id column to string if necessary\n",
    "clean_data[\"row_id\"] = clean_data[\"row_id\"].astype(str)\n",
    "\n",
    "# Function to filter and save the dataset based on row indices\n",
    "def filter_and_save(category, clean_df, row_ids):\n",
    "    \"\"\"\n",
    "    Filters the clean dataset based on the row IDs provided in the JSON and saves the filtered dataset.\n",
    "\n",
    "    Args:\n",
    "        category (str): Name of the dataset category.\n",
    "        clean_df (pd.DataFrame): Clean dataset DataFrame.\n",
    "        row_ids (set): Set of row IDs to retain.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    filtered_data = clean_df[clean_df[\"row_id\"].isin(row_ids)]\n",
    "    output_filename = os.path.join(DATASET_DIR, f\"test_{category}.csv\")\n",
    "    filtered_data.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved {output_filename} with {len(filtered_data)} rows.\")\n",
    "\n",
    "\n",
    "# Process each dataset category in the JSON file\n",
    "for category, indices in row_indices.items():\n",
    "    filter_and_save(category, clean_data, set(indices))\n",
    "\n",
    "print(\"All filtered datasets have been successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils4\n",
    "`Utils4` is a script designed to reconstruct test datasets by extracting relevant rows from clean datasets based on precomputed row pair information. The script loads row pairs stored in JSON files, filters the corresponding clean datasets, and saves the extracted test subsets for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ beers: 从 JSON 提取了 157 个唯一行号\n",
      "📁 beers: 生成测试集 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/test_beers.csv, 共 157 条记录\n",
      "✅ flight: 从 JSON 提取了 450 个唯一行号\n",
      "📁 flight: 生成测试集 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/test_flight.csv, 共 450 条记录\n",
      "✅ rayyan: 从 JSON 提取了 132 个唯一行号\n",
      "📁 rayyan: 生成测试集 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/test_rayyan.csv, 共 132 条记录\n",
      "✅ restaurant: 从 JSON 提取了 219 个唯一行号\n",
      "📁 restaurant: 生成测试集 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/test_restaurant.csv, 共 219 条记录\n",
      "✅ soccer: 从 JSON 提取了 1999 个唯一行号\n",
      "📁 soccer: 生成测试集 /home/liuxinyuan/table_tuning_for_error_generating_task/evaluation/test_dataset/exp_1/raw_test_dataset/test_soccer.csv, 共 1999 条记录\n",
      "🎯 所有数据集处理完成！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base directory relative to the script's location\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"evaluation\", \"test_dataset\", \"exp_1\", \"raw_test_dataset\")\n",
    "\n",
    "# List of datasets to process\n",
    "datasets = [\"beers\", \"flight\", \"rayyan\", \"Marketing\", \"Company\"]\n",
    "\n",
    "# Function to extract test dataset based on row pairs from JSON\n",
    "def extract_test_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Extracts a test dataset based on row pairs specified in the JSON file.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    clean_file_path = os.path.join(DATASET_DIR, f\"clean_{dataset}.csv\")\n",
    "    test_json_path = os.path.join(DATASET_DIR, f\"row_pairs_{dataset}.json\")\n",
    "\n",
    "    # Load clean dataset\n",
    "    clean_data = pd.read_csv(clean_file_path, dtype=str)\n",
    "\n",
    "    # Load row pairs from JSON\n",
    "    with open(test_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        row_pairs = json.load(f)\n",
    "\n",
    "    # Extract unique row IDs\n",
    "    row_ids = {row_id for pairs in row_pairs.values() for row_id in pairs}\n",
    "\n",
    "    print(f\"✔ {dataset}: Extracted {len(row_ids)} unique row IDs from JSON.\")\n",
    "\n",
    "    # Filter dataset based on extracted row IDs\n",
    "    test_df = clean_data[clean_data[\"row_id\"].isin(row_ids)].copy()\n",
    "\n",
    "    # Convert row_id column to integer\n",
    "    test_df[\"row_id\"] = test_df[\"row_id\"].astype(int)\n",
    "\n",
    "    # Save the extracted test dataset\n",
    "    output_file = os.path.join(DATASET_DIR, f\"test_{dataset}.csv\")\n",
    "    test_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"{dataset}: Test dataset saved at {output_file}, containing {test_df.shape[0]} records.\")\n",
    "\n",
    "# Process each dataset\n",
    "for dataset in datasets:\n",
    "    extract_test_dataset(dataset)\n",
    "\n",
    "print(\"All datasets have been successfully processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Utils5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the base directory relative to the script location\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"evaluation\", \"test_dataset\", \"exp_1\", \"BART_output\", \"beers\")\n",
    "\n",
    "# Define file paths for clean and dirty datasets\n",
    "clean_file_path = os.path.join(DATASET_DIR, \"test_beers.csv\")\n",
    "dirty_file_path = os.path.join(DATASET_DIR, \"bears_10\", \"dirty_data.csv\")\n",
    "output_json_path = os.path.join(DATASET_DIR, \"differences.json\")\n",
    "\n",
    "# Load clean and dirty datasets\n",
    "clean_df = pd.read_csv(clean_file_path, dtype=str)\n",
    "dirty_df = pd.read_csv(dirty_file_path, dtype=str)\n",
    "\n",
    "# Ensure both DataFrames have the same shape\n",
    "if clean_df.shape != dirty_df.shape:\n",
    "    raise ValueError(\"The shapes of clean and dirty datasets do not match. Please check the data!\")\n",
    "\n",
    "# Initialize a list to store detected inconsistencies\n",
    "differences = []\n",
    "\n",
    "# Compare each cell in the DataFrames\n",
    "for row in range(clean_df.shape[0]):\n",
    "    for col in range(clean_df.shape[1]):\n",
    "        clean_value = clean_df.iloc[row, col]\n",
    "        dirty_value = dirty_df.iloc[row, col]\n",
    "\n",
    "        # Record the differences if values are inconsistent\n",
    "        if clean_value != dirty_value:\n",
    "            differences.append({\n",
    "                \"row\": row,\n",
    "                \"column\": col,\n",
    "                \"right_value\": clean_value,\n",
    "                \"error_value\": dirty_value\n",
    "            })\n",
    "\n",
    "# Print detected differences\n",
    "print(\"Detected inconsistent cells:\")\n",
    "for diff in differences:\n",
    "    print(diff)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(differences, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Inconsistent cells have been saved to: {output_json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuning_for_egtask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
